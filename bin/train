#!/bin/python
# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------
import sys

sys.path.append('./src')

from pytorch_common.callbacks import SaveBestModel, EarlyStop, ReduceLROnPlateau
from pytorch_common.callbacks.output import Logger
from pytorch_common.util import get_device, set_device_name, trainable_params_count

import logging

from torch import nn
from torch.optim import Adam

from model import Loss, AttentionSeqToSeqModel, ModelManager

from logger import initialize_logger
import click

from torchtext.legacy.data import BucketIterator

from data import DatasetLoader
from field_factory import FieldFactory


# -----------------------------------------------------------------------------
#
#
#
#
#
# -----------------------------------------------------------------------------
# Functions
# -----------------------------------------------------------------------------
def log_hyper_params(batch_size, dataset_path, device, dropout, early_stop_patience, epochs, learning_rate,
                     origin_language_model, origin_min_freq, reduce_lr_factor, reduce_lr_min, reduce_lr_patience,
                     rnn_hidden_state_dim, source_embedding_dim, target_embedding_dim, target_language_model,
                     target_min_freq):
    logging.info('Hyper parameters:')
    logging.info(' - dataset_path: {}'.format(dataset_path))
    logging.info(' - batch_size: {}'.format(batch_size))
    logging.info(' - origin_language_model: {}'.format(origin_language_model))
    logging.info(' - target_language_model: {}'.format(target_language_model))
    logging.info(' - origin_min_freq: {}'.format(origin_min_freq))
    logging.info(' - target_min_freq: {}'.format(target_min_freq))
    logging.info(' - device: {}'.format(device))
    logging.info(' - source_embedding_dim: {}'.format(source_embedding_dim))
    logging.info(' - target_embedding_dim: {}'.format(target_embedding_dim))
    logging.info(' - rnn_hidden_state_dim: {}'.format(rnn_hidden_state_dim))
    logging.info(' - dropout: {}'.format(dropout))
    logging.info(' - learning_rate: {}'.format(learning_rate))
    logging.info(' - epochs: {}'.format(epochs))
    logging.info(' - early_stop_patience: {}'.format(early_stop_patience))
    logging.info(' - reduce_lr_patience: {}'.format(reduce_lr_patience))
    logging.info(' - reduce_lr_factor: {}'.format(reduce_lr_factor))
    logging.info(' - reduce_lr_min: {}'.format(reduce_lr_min))


# -----------------------------------------------------------------------------
#
#
#
#
#
# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
@click.command()
@click.option('--dataset-path', default='./dataset', help='Dataset path (Default: ./dataset)')
@click.option('--batch-size', default=128, help='Batch size (Default: 128)')
@click.option('--origin-language-model', default='de_core_news_sm', help='Origin language (Default: de_core_news_sm')
@click.option('--target-language-model', default='en_core_web_sm', help='Target language (Default: en_core_web_sm)')
@click.option('--origin-min-freq', default=2, help='Origin language word min frequency (Default: 2)')
@click.option('--target-min-freq', default=2, help='Target language word min frequency (Default: 2)')
@click.option('--source-embedding-dim', default=256, help='Source language embedding dimension (Default: 256)')
@click.option('--target-embedding-dim', default=256, help='Target language embedding dimension (Default: 256)')
@click.option('--rnn-hidden-state-dim', default=256, help='Rnn hidden state dimension (Default: 256)')
@click.option('--dropout', default=0.5, help='Dropout (Default: 0.5)')
@click.option('--learning-rate', default=0.001, help='Learning rate (Default: 0.001)')
@click.option('--epochs', default=20, help='Epochs (Default: 20)')
@click.option('--reduce-lr-patience', default=4, help='Reduce learning rate on plateau patience (Default: 4)')
@click.option('--reduce-lr-factor', default=0.0015, help='Reduce learning rate on plateau factor (Default: 0.0015)')
@click.option('--reduce-lr-min', default=0.0001, help='Reduce learning rate on plateau factor (Default: 0.0001)')
@click.option('--early-stop-patience', default=5, help='Early stop patience (Default: 5)')
@click.option(
    '--device',
    default='gpu',
    help='Device used to train and optimize model. Values: gpu(Default) or cpu(Fallback).'
)
def main(
        dataset_path,
        batch_size,
        origin_language_model,
        target_language_model,
        origin_min_freq,
        target_min_freq,
        device,
        source_embedding_dim,
        target_embedding_dim,
        rnn_hidden_state_dim,
        dropout,
        learning_rate,
        epochs,
        early_stop_patience,
        reduce_lr_patience,
        reduce_lr_factor,
        reduce_lr_min
):
    initialize_logger('%(asctime)s %(levelname)-1s %(message)s')
    log_hyper_params(batch_size, dataset_path, device, dropout, early_stop_patience, epochs, learning_rate,
                     origin_language_model, origin_min_freq, reduce_lr_factor, reduce_lr_min, reduce_lr_patience,
                     rnn_hidden_state_dim, source_embedding_dim, target_embedding_dim, target_language_model,
                     target_min_freq)

    set_device_name(device)

    source_field = FieldFactory.create(origin_language_model)
    target_field = FieldFactory.create(target_language_model)

    loader = DatasetLoader(source_field, target_field)

    train_data = loader.load(f'{dataset_path}/train.json')
    valid_data = loader.load(f'{dataset_path}/valid.json')

    train_iterator, valid_iterator = BucketIterator.splits(
        (train_data, valid_data),
        batch_size=batch_size,
        sort=False,
        device=get_device()
    )

    source_field.build_vocab(train_data, min_freq=origin_min_freq)
    target_field.build_vocab(train_data, min_freq=target_min_freq)

    model = AttentionSeqToSeqModel(
        source_vocab_dim=len(source_field.vocab),
        target_vocab_dim=len(target_field.vocab),
        enc_embedding_dim=source_embedding_dim,
        dec_embedding_dim=target_embedding_dim,
        enc_dropout=dropout,
        dec_dropout=dropout,
        enc_hidden_state_dim=rnn_hidden_state_dim,
        dec_hidden_state_dim=rnn_hidden_state_dim
    ).init_weights().to(get_device())

    logging.info(f'Model:\n{model}\nTrainable params: {trainable_params_count(model)}')

    model_manager = ModelManager(
        model,
        optimizer=Adam(model.parameters(), lr=learning_rate),
        loss_fn=Loss(
            loss_fn=nn.CrossEntropyLoss(ignore_index=target_field.vocab.stoi[target_field.pad_token]),
            target_vocab_dim=len(target_field.vocab)
        )
    )

    logging.info(f'Begin model training...')
    model_manager.fit(
        train_iterator,
        valid_iterator,
        epochs=epochs,
        callbacks=[
            SaveBestModel(metric='val_loss'),
            ReduceLROnPlateau(
                patience=reduce_lr_patience,
                metric='val_loss',
                factor=reduce_lr_factor,
                min_lr=reduce_lr_min
            ),
            Logger(metrics=['time', 'epoch', 'train_loss', 'val_loss', 'patience', 'lr']),
            EarlyStop(metric='val_loss', patience=early_stop_patience, mode='min')
        ]
    )


if __name__ == '__main__':
    main()
# -----------------------------------------------------------------------------
